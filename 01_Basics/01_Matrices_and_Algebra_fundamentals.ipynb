{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01_Matrices and Algebra fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Objects\n",
    "\n",
    "The basic mathematical objects we have to understand as data scientist are the following picture. The two primary mathematical entities that are of interest in linear algebra are the vector and the matrix. They are examples of a more general entity known as a tensor. Tensors possess an order (or rank), which determines the number of dimensions in an array required to represent it.:\n",
    "![matrix-image](https://miro.medium.com/max/700/0*sjDnWS2QtJUa0Gy8.png)\n",
    "\n",
    "The two primary mathematical entities that are of interest in linear algebra are the vector and the matrix. They are examples of a more general entity known as a tensor. Tensors possess an order (or rank), which determines the number of dimensions in an array required to represent it.\n",
    "\n",
    "### <a href=\"https://en.wikipedia.org/wiki/Scalar_(mathematics)\" target=\"_top\">Scalar:</a>\n",
    "\n",
    "\n",
    "__Scalars__ are single numbers and are an example of a __0th-order tensor__.  \n",
    "\n",
    "In mathematics it is necessary to describe the set of values to which a scalar belongs.  The notation  $x \\in \\mathbb{R}$ states that the (lowercase) scalar value $x$ is an element of (or member of) the set of real-valued numbers, $\\mathbb{R}$.\n",
    "\n",
    "There are various sets of numbers of interest within machine learning.  $\\mathbb{N}$ represents the set of positive integers ($1,2,3,...$).  $\\mathbb{Z}$represents the integers, which include positive, negative and zero values.  $\\mathbb{Q}$ represents the set of rational numbers that may be expressed as a fraction of two integers.\n",
    "\n",
    "In the graphic above the example for an scalar is the single number 24. \n",
    "\n",
    "### <a href=\"https://en.wikipedia.org/wiki/Vector_(mathematics_and_physics)\" target=\"_top\">Vector:</a>\n",
    "\n",
    "__Vectors__ are ordered arrays of single numbers and are an example of __1st-order tensor__. Vectors are members of objects known as __vector spaces__. \n",
    "\n",
    "A __vector space__ can be thought of as the entire collection of all possible vectors of a particular length (or dimension). The three-dimensional real-valued vector space, denoted by $\\mathbb{R}^3$ is often used to represent our real-world notion of three-dimensional space mathematically.\n",
    "\n",
    "More formally a vector space is an $n$-dimensional [Cartesian product](https://en.wikipedia.org/wiki/Cartesian_product) of a set with itself, along with proper definitions on how to add vectors and multiply them with scalar values. If all of the scalars in a vector are real-valued then the notation $x \\in \\mathbb{R}^n $ states that the (boldface lowercase) vector value $x$ is a member of the $n$-dimensional vector space of real numbers, $\\mathbb{R}^n$.\n",
    "\n",
    "Sometimes it is necessary to identify the _components_ of a vector explicitly. The $i$th scalar element of a vector is written as $x_i$ . Notice that this is non-bold lowercase since the element is a scalar. An $n$-dimensional vector itself can be explicitly written using the following notation:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\boldsymbol{x}=\\begin{bmatrix}\n",
    "  \\kern4pt x_1 \\kern4pt \\\\\n",
    "  \\kern4pt x_2 \\kern4pt \\\\\n",
    "  \\kern4pt \\vdots \\kern4pt \\\\\n",
    "  \\kern4pt x_n \\kern4pt\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Given that __scalars__ exist to represent values why are vectors necessary? One of the primary use cases for vectors is to represent physical quantities that have both a magnitude and a direction. Scalars are only capable of representing magnitudes.\n",
    "\n",
    "![vector-representing](https://www.mathsisfun.com/algebra/images/vector-mag-dir.svg)\n",
    "\n",
    "For instance __scalars__ and __vectors__ encode the difference between the _speed_ of a car and its _velocity_. The velocity contains not only its speed but also its direction of travel. It is not difficult to imagine many more physical quantities that possess similar characteristics such as gravitational and electromagnetic forces or wind velocity.\n",
    "\n",
    "In machine learning vectors often represent _feature vectors_, with their individual components specifying how important a particular feature is. Such features could include relative importance of words in a text document, the intensity of a set of pixels in a two-dimensional image or historical price values for a cross-section of financial instruments.\n",
    "\n",
    "> Simply expressed a __Vector__ is an ordered array of numbers and can be in __a row or a column__. A Vector has just a __single index__, which can point to a specific value within the Vector. For example, V2 refers to the second value within the Vector, which is -8 in the inital graphic above.\n",
    "\n",
    "### <a href=\"https://en.wikipedia.org/wiki/Matrix_(mathematics)\" target=\"_top\">Matrices:</a>\n",
    "\n",
    "__Matrices__ are rectangular arrays consisting of numbers and are an example of __2nd-order tensors__. \n",
    "\n",
    "If $m$ and $m$ are positive integers, that is $m,n \\in \\mathbb{N}$ then the  matrix contains $mn$ numbers, with $m$ rows and $n$ columns.\n",
    "\n",
    "If all of the __scalars__ in a __matrix__ are real-valued then a matrix is denoted with uppercase boldface letters, such as $ \\boldsymbol{A} \\in \\mathbb{R}^{m \\times n}$ . That is the matrix lives in a $m \\times n$-dimensional real-valued vector space. Hence matrices are really vectors that are just written in a two-dimensional table-like manner.\n",
    "\n",
    "Its components are now identified by two indices $i$ and $j$. $i$ represents the index to the matrix row, while $j$ represents the index to the matrix column. Each component of $\\boldsymbol{A}$ is identified by $a_{ij}$.\n",
    "\n",
    "The full $m \\times n$ matrix can be written as:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\boldsymbol{A}=\\begin{bmatrix}\n",
    "  \\kern4pt a_{11} & a_{12} & a_{13} & \\ldots & a_{1n} \\kern4pt \\\\\n",
    "  \\kern4pt a_{21} & a_{22} & a_{23} & \\ldots & a_{2n} \\kern4pt \\\\\n",
    "  \\kern4pt a_{31} & a_{32} & a_{33} & \\ldots & a_{3n} \\kern4pt \\\\\n",
    "  \\kern4pt \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\kern4pt \\\\\n",
    "  \\kern4pt a_{m1} & a_{m2} & a_{m3} & \\ldots & a_{mn} \\kern4pt \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "It is often useful to abbreviate the full matrix component display into the following expression:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\boldsymbol{A} = [a_{ij}]_{m \\times n}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Where $a_{ij}$ is referred to as the -element of the __matrix__ $\\boldsymbol{A}$. The subscript of $m \\times n$ can be dropped if the dimension of the matrix is clear from the context.\n",
    "\n",
    "Note that a _column vector_ is a size $m \\times 1$ matrix, since it has $m$ rows and 1 column. Unless otherwise specified all vectors will be considered to be column vectors.\n",
    "\n",
    "__Matrices__ represent a type of function known as a [linear map](https://en.wikipedia.org/wiki/Linear_map). Based on rules that will be outlined in subsequent articles, it is possible to define multiplication operations between matrices or between matrices and vectors. Such operations are immensely important across the physical sciences, quantitative finance, computer science and machine learning.\n",
    "\n",
    "__Matrices__ can encode geometric operations such as rotation, reflection and transformation. Thus if a collection of vectors represents the vertices of a three-dimensional geometric model in [Computer Aided Design](https://en.wikipedia.org/wiki/Computer-aided_design) software then multiplying these vectors individually by a pre-defined [rotation matrix](https://en.wikipedia.org/wiki/Rotation_matrix) will output new vectors that represent the locations of the rotated vertices. This is the basis of modern 3D computer graphics.\n",
    "\n",
    "In __deep learning neural network__ weights are stored as __matrices__, while feature inputs are stored as __vectors__. Formulating the problem in terms of linear algebra allows compact handling of these computations. By casting the problem in terms of __tensors__ and utilising the machinery of linear algebra, rapid training times on modern GPU hardware can be obtained.\n",
    "\n",
    "\n",
    "> Summarized and in sinple words a __Matrix__ is an ordered 2D array of numbers and it has __two indices__. The first one points to the row and the second one to the column. For example, M23 refers to the value in the second row and the third column, which is 8 in the inital yellow graphic above at the beginning. A Matrix can have __multiple numbers of rows and columns__. Note that a __Vector__ is also a Matrix, but with only one row or one column.\n",
    "\n",
    "![matrix-image](https://upload.wikimedia.org/wikipedia/commons/b/bb/Matrix.svg)\n",
    "\n",
    "### <a href=\"https://en.wikipedia.org/wiki/Tensor\" target=\"_top\">Tensors:</a>\n",
    "\n",
    "The more general entity of a __tensor__ encapsulates the __scalar__, __vector__ and the __matrix__. It is sometimes necessary—both in the physical sciences and machine learning—to make use of tensors with order that exceeds two.\n",
    "\n",
    "In theoretical physics, and general relativity in particular, the [Riemann curvature tensor](https://en.wikipedia.org/wiki/Riemann_curvature_tensor) is a __4th-order tensor__ that describes the local curvature of [spacetime](https://en.wikipedia.org/wiki/Spacetime). In machine learning, and deep learning in particular, a __3rd-order tensor__ can be used to describe the intensity values of multiple channels (red, green and blue) from a two-dimensional image.\n",
    "\n",
    "__Tensors__ will be identified in this series of posts via the boldface sans-serif notation, $\\textsf{A}$. For a __3rd-order tensor__ elements will be given by $a_{ijk}$, whereas for a __4th-order tensor__ elements will be given by $a_{ijkl}$.\n",
    "\n",
    "The following graphic should summarize and help to understand the relationships between scalar, vector, matrix and tensor.\n",
    "\n",
    "![matrix-image](https://cdn-images-1.medium.com/max/880/1*WbLIc4-xIOfHiO2oWzimyA.png)\n",
    "\n",
    "> __Tensor__ is the most general term for all of these concepts above because a Tensor is a multidimensional array and it can be a __Vector__ and a __Matrix__, depending on the __number of indices__ it has. For example, a __first-order Tensor__ would be a __Vector__ (1 index). A __second-order Tensor__ is a __Matrix__ (2 indices) and __third-order Tensors__ (3 indices) and higher are called __Higher-Order Tensors__ (3 or more indices)\n",
    "\n",
    "### Sources: \n",
    "* [Scalars, Vectors, Matrices and Tensors - Linear Algebra for Deep Learning](https://www.quantstart.com/articles/scalars-vectors-matrices-and-tensors-linear-algebra-for-deep-learning-part-1/#:~:text=Vectors%20and%20Matrices,array%20required%20to%20represent%20it)\n",
    "* [Basic Linear Algebra for Deep Learning](https://towardsdatascience.com/linear-algebra-for-deep-learning-f21d7e7d7f23)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Matrix Operations\n",
    "\n",
    "At this point we will look on how to form operations between the entities we looked above. Such operations include addition and multiplication. While you will be very familiar with scalar addition and multiplication, the rules differ somewhat when dealing with more general tensor entities. \n",
    "\n",
    "We will begin by looking at __matrix addition__ and then consider __matrix multiplication__. These operations will eventually allow us to discuss a topic known as __matrix inversion__.\n",
    "\n",
    "### [Matrix Addition](https://en.wikipedia.org/wiki/Matrix_addition):\n",
    "\n",
    "What does it mean to add two matrices together? In this section we will explore such an operation and hopefully see that it is actually quite intuitive.\n",
    "\n",
    "Matrices can be added to __scalars__, __vectors__ and __other matrices__. Each of these operations has a precise definition. These techniques are used frequently in machine learning and deep learning so it is worth familiarising yourself with them.\n",
    "\n",
    "#### Matrix-Matrix Addition\n",
    "\n",
    "Given two matrices of size $m \\times n$, $\\boldsymbol{A}=[a_{ij}]$ and $\\boldsymbol{B}=[b_{ij}]$, it is possible to define the matrix $\\boldsymbol{C}=[c_{ij}]$ as the __matrix sum__ $\\boldsymbol{C} = \\boldsymbol{A} + \\boldsymbol{B}$ where $c_{ij} = a_{ij} + b_{ij}$.\n",
    "\n",
    "That is, $\\boldsymbol{C}$ is constructed by element-wise summing the respective elements of $\\boldsymbol{A}$ and $\\boldsymbol{B}$. This operation is only defined where the two matrices have equal size, except in the case of __broadcasting__ below. The definition implies that $\\boldsymbol{C}$ also has size $m \\times n$.\n",
    "\n",
    "Matrix addition is __[commutative](https://en.wikipedia.org/wiki/Commutative_property)__. This means that it doesn't matter which way around the matrices are added:\n",
    "\n",
    "![Commutative Property](https://www.onlinemathlearning.com/image-files/commutative-property.png)\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\boldsymbol{A} + \\boldsymbol{B} = \\boldsymbol{B} + \\boldsymbol{A}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "It is also __[associative](https://en.wikipedia.org/wiki/Associative_property)__. This means that you get the same result if you add two matrices together first, and then another, as if you add another two together first and then the other:\n",
    "\n",
    "![Associative Property](https://www.onlinemathlearning.com/image-files/associative-property.png)\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\boldsymbol{A} + (\\boldsymbol{B} + \\boldsymbol{C}) = (\\boldsymbol{A} + \\boldsymbol{B}) + \\boldsymbol{C}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Both of these results follow from the fact that normal scalar addition is itself __commutative__ and __associative__, because we're just adding the elements together.\n",
    "\n",
    "__Example__\n",
    "\n",
    "Consider two matrices $\\boldsymbol{A}$ and $\\boldsymbol{B}$.\n",
    "\n",
    "![matrix-image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0fb7845af355c550d44020e52699069c044126a9)\n",
    "\n",
    "And now we will use random numbers in our example. We can create a new matrix $\\boldsymbol{C}$ through addition::\n",
    "\n",
    "$$\n",
    " \\boldsymbol{A} + \\boldsymbol{B} = \\left[ \\begin{array}{ccc}\n",
    "1 & 4 & 17 \\\\\n",
    "18 & 3 & 2 \\\\\n",
    "5 & 19 & 1 \\\\\n",
    "\\end{array} \\right] +\n",
    "\\left[ \\begin{array}{ccc}\n",
    "12 & 18 & 6 \\\\\n",
    "4 & 3 & 33 \\\\\n",
    "23 & 5 & 8 \\\\\n",
    "\\end{array} \\right] = \n",
    "\\left[ \\begin{array}{ccc}\n",
    "13 & 22 & 23 \\\\\n",
    "22 & 6 & 35 \\\\\n",
    "28 & 24 & 9 \\\\\n",
    "\\end{array} \\right] = \n",
    "\\boldsymbol{C}\n",
    "$$\n",
    " \n",
    "It is clear to see that the elements of $\\boldsymbol{C}$ are simply the elements added in the corresponding positions from $\\boldsymbol{A}$ and $\\boldsymbol{B}$.\n",
    "\n",
    "#### Matrix-Scalar Addition\n",
    "\n",
    "It is possible to add a scalar value $x$ to a matrix $\\boldsymbol{A}=[a_{ij}]$ to produce a new matrix $\\boldsymbol{B}=[b_{ij}]$ where $b_{ij} = x + a_{ij}$ . This simply means that we're adding the same scalar value to every element of the matrix. It is written as $\\boldsymbol{B} = x + \\boldsymbol{A}$.\n",
    "\n",
    "Scalar-matrix addition is once again __commutative__ and __associative__, because normal scalar addition is both commutative and associative.\n",
    "\n",
    "Here a example with numbers:\n",
    "\n",
    "![Matrix-Scalar Addition-image](https://miro.medium.com/max/316/1*B14w7k62qe521VoTxvMiHA.png)\n",
    "\n",
    "#### Broadcasting\n",
    "\n",
    "For certain applications in machine learning it is possible to define a shorthand notation known as __broadcasting__. Consider $\\boldsymbol{A} \\in \\mathbb{R}^{m \\times n}$ an $m \\times n$-dimensional real-valued matrix and \n",
    "$\\boldsymbol{x} \\in \\mathbb{R}^m$ an $m$-dimensional vector.\n",
    "\n",
    "It is possible to define $\\boldsymbol{B} = \\boldsymbol{A} + \\boldsymbol{x}$, despite the fact that the matrices $\\boldsymbol{A}$ and $\\boldsymbol{x}$ are unequal in size. The definition of the sum takes $b_{ij} = a_{ij} + x_j$.\n",
    "\n",
    "\n",
    "That is, the elements of $\\boldsymbol{x}$ are copied into each row of the matrix $\\boldsymbol{A}$. Since the value of $\\boldsymbol{x}$ is __broadcast__ into each row the process is known as broadcasting.\n",
    "\n",
    "### [Matrix Multiplication](https://en.wikipedia.org/wiki/Scalar_multiplication)\n",
    "\n",
    "The rules for matrix addition are relatively simple and intuitive. However when it comes to multiplication of matrices the rules become more complex.\n",
    "\n",
    "#### [Matrix Transpose](https://en.wikipedia.org/wiki/Transpose)\n",
    "\n",
    "In order to define certain matrix-matrix multiplication operations such as the __dot-product__ (discussed below) it is necessary to define the __transpose__ of a matrix. The transpose of a matrix $\\boldsymbol{A}=[a_{ij}]_{m \\times n}$ of size $m \\times n$ is denoted by $\\boldsymbol{A}^{T}$ of size $n \\times m$ and is given element-wise by the following expression:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\boldsymbol{A}^{T} = [a_{ji}]_{n \\times m}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "That is, the indices $i$ and $j$ are swapped. This has the effect of reflecting the matrix along the line of diagonal elements $a_{ii}$. The operation is defined for non-square matrices, as well as vectors and scalars (which are simply $1 \\times 1$ matrices). Note that a scalar is its own transpose: $x = x^T$. In addition the transpose of the transpose of a matrix is simply itself: $\\boldsymbol{A}^{TT} = \\boldsymbol{A}$.\n",
    "\n",
    "__Examples__\n",
    "$$\n",
    " \\boldsymbol{A} = \\left[ \\begin{array}{ccc}\n",
    "a_{11} & a_{12} & a_{13} \\\\\n",
    "a_{21} & a_{22} & a_{23}\n",
    "\\end{array} \\right], \\quad\n",
    "\\boldsymbol{A}^T = \\left[ \\begin{array}{cc}\n",
    "a_{11} & a_{21} \\\\\n",
    "a_{12} & a_{22} \\\\\n",
    "a_{13} & a_{23}\n",
    "\\end{array} \\right]\n",
    "$$\n",
    "\n",
    "$$ \\boldsymbol{x} = \\left[ \\begin{array}{c}\n",
    "x_{1} \\\\\n",
    "x_{2} \\\\\n",
    "x_{3}\n",
    "\\end{array} \\right], \\quad\n",
    "\\boldsymbol{x}^T = \\left[ \\begin{array}{ccc}\n",
    "x_{1} & x_{2} & x_3\n",
    "\\end{array} \\right]\n",
    "$$ \n",
    " \n",
    "Note here that $\\boldsymbol{A}^{T}$ does not represent $\\boldsymbol{A}$ multiplied by itself $\\boldsymbol{T}$ times. This is an entirely different operation. However, it will usually be clear from the context whether we mean the transpose of a matrix or repeated multiplication by itself.\n",
    "\n",
    "#### [Matrix-Matrix Multiplication](https://en.wikipedia.org/wiki/Matrix_multiplication)\n",
    "\n",
    "We are now going to consider __matrix-matrix multiplication__. This is a more complex operation than __matrix addition__ because it does not simply involve multiplying the matrices element-wise. Instead a more complex procedure is utilised, for each element, involving an entire row of one matrix and an entire column of the other.\n",
    "\n",
    "The operation is only defined for matrices of specific sizes. The first matrix must have as many columns as the second matrix has rows, otherwise the operation is not defined.\n",
    "\n",
    "The definition below can be a bit tricky to understand initially, so have a look at it first and then try working through the examples to see how specific numeric instances match up to the general formula.\n",
    "\n",
    "Given a matrix $\\boldsymbol{A}=[a_{ij}]_{m \\times n}$ and a matrix $\\boldsymbol{B}=[b_{ij}]_{n \\times p}$ the __matrix product__ $\\boldsymbol{C}=\\boldsymbol{A}\\boldsymbol{B}=[c_{ij}]_{m \\times p}$ is defined element-wise by:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "c_{ij} = \\sum^n_{k=1} a_{ik} b_{kj}\n",
    "\\end{equation}\n",
    "$$\n",
    " \n",
    "That is the elements $c_{ij}$ of the matrix $\\boldsymbol{C}=\\boldsymbol{A}\\boldsymbol{B}$ are given by summing the products of the elements of the $i$-th row of $\\boldsymbol{A}$ with the elements of the $j$-th column of $\\boldsymbol{B}$.\n",
    "\n",
    "Note that matrix-matrix multiplication is not commutative. That is:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{A}\\boldsymbol{B} \\neq \\boldsymbol{B}\\boldsymbol{A}\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "__Examples__\n",
    "\n",
    "Given the following two matrices:\n",
    "\n",
    "$$\n",
    " \\boldsymbol{A} = \\left[ \\begin{array}{ccc}\n",
    "2 & 5 & 1 \\\\\n",
    "7 & 3 & 6\n",
    "\\end{array} \\right], \\quad\n",
    "\\boldsymbol{B} = \\left[ \\begin{array}{cc}\n",
    "1 & 8 \\\\\n",
    "9 & 4 \\\\\n",
    "3 & 5\n",
    "\\end{array} \\right]\n",
    "$$\n",
    " \n",
    " \n",
    "It is possible to construct the product $\\boldsymbol{A}\\boldsymbol{B}$ of size $2 \\times 2$:\n",
    "\n",
    "$$ \n",
    "\\boldsymbol{AB} = \\left[ \\begin{array}{cc}\n",
    "2 \\cdot 1 + 5 \\cdot 9 + 1 \\cdot 3 & 2 \\cdot 8 + 5 \\cdot 4 + 1 \\cdot 5 \\\\\n",
    "7 \\cdot 1 + 3 \\cdot 9 + 6 \\cdot 3 & 7 \\cdot 8 + 3 \\cdot 4 + 6 \\cdot 5\n",
    "\\end{array} \\right] =\n",
    "\\left[ \\begin{array}{cc}\n",
    "50 & 41 \\\\\n",
    "52 & 98\n",
    "\\end{array} \\right]\n",
    "$$\n",
    "  \t\t\t \n",
    "It is also possible to construct the product $\\boldsymbol{B}\\boldsymbol{A}$ of size $3 \\times 3$:\n",
    "\n",
    "$$\n",
    " \\boldsymbol{BA} = \\left[ \\begin{array}{ccc}\n",
    "1 \\cdot 2 + 8 \\cdot 7 & 1 \\cdot 5 + 8 \\cdot 3 & 1 \\cdot 1 + 8 \\cdot 6 \\\\\n",
    "9 \\cdot 2 + 4 \\cdot 7 & 9 \\cdot 5 + 4 \\cdot 3 & 9 \\cdot 1 + 4 \\cdot 6 \\\\\n",
    "3 \\cdot 2 + 5 \\cdot 7 & 3 \\cdot 5 + 5 \\cdot 3 & 3 \\cdot 1 + 5 \\cdot 6 \\\\\n",
    "\\end{array} \\right] =\n",
    "\\left[ \\begin{array}{ccc}\n",
    "58 & 29 & 49 \\\\\n",
    "46 & 57 & 33 \\\\\n",
    "41 & 30 & 33\n",
    "\\end{array} \\right]\n",
    "$$\n",
    "  \n",
    "The above definition does not initially seem to be motivated in a simple way. Why is matrix-matrix multiplication defined like this? It has to do with a deeper result __involving matrices representing linear map functions__ between two vector spaces. We need not worry about these certain deeper aspects of linear maps as they are beyond the scope of this article series.\n",
    "\n",
    "However, we can briefly provide some intuition through the idea of composing functions together. It is common in mathematics to compose two functions together to produce a new function. That is the function $f$ can be defined as $h = f \\circ g$, with the $\\circ$ symbol representing function composition. This notation means that $h$ is equivalent to $g$ being carried out first, and then $f$.\n",
    "\n",
    "If, for example $f=sin(x)$ and $g=x^2$ then $h = sin(x^2)$\n",
    ". Function composition is not commutative and as such $f \\circ g \\neq g \\circ f$. Instead $g \\circ f = sin^2 (x)$, which is a completely different function. This is why matrix-matrix multiplication is not commutative and also why it is defined as above.\n",
    "\n",
    "Note also that this definition does not imply that the elements of the matrix $\\boldsymbol{C}$ are defined as the element-wise multiplication of those from $\\boldsymbol{A}$ and $\\boldsymbol{B}$. That is, the elements in each location cannot simply be multiplied together to get the new product matrix. That is an entirely different operation known as the __[Hadamard Product](https://en.wikipedia.org/wiki/Hadamard_product_(matrices))__ and will be discussed below.\n",
    "\n",
    "Since a column vector is in fact a $n \\times 1$ matrix it is possible to carry out matrix-vector multiplication. If the left-multiplying matrix has size $m \\times n$ then this is a valid operation that will produce another $m \\times 1$ matrix (column vector) as output.\n",
    "\n",
    "Matrix-matrix and matrix-vector multiplication are extremely common operation in the physical sciences, computational graphics and machine learning fields. As such highly optimised software libraries such as [BLAS](https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms) and [LAPACK](https://en.wikipedia.org/wiki/LAPACK) have been developed to allow efficient scientific computation--particularly on [GPUs](https://en.wikipedia.org/wiki/Graphics_processing_unit).\n",
    "\n",
    "#### [Scalar-Matrix Multiplication](https://en.wikipedia.org/wiki/Scalar_multiplication)\n",
    "\n",
    "Scalar-matrix multiplication is simpler than matrix-matrix multiplication. Given a matrix $\\boldsymbol{A}=[a_{ij}]_{m \\times n}$ and a scalar $\\lambda \\in \\mathbb{R}$ the scalar-matrix product $\\lambda \\boldsymbol{A}$ is calculated by multiplying every element of $\\boldsymbol{A}$ by $\\lambda$ such that $\\lambda \\boldsymbol{A} = [\\lambda a_{ij}]_{m \\times n}$.\n",
    "\n",
    "![Matrix-Scalar Multiplication-image](https://wikimedia.org/api/rest_v1/media/math/render/svg/58186145172bc4ad9bb672c620495f6508b88af3)\n",
    "\n",
    "If we take two real-valued scalars $\\lambda, \\mu \\in \\mathbb{R}$ the subsequent useful relationships follow from the definition above:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\lambda (\\boldsymbol{A} + \\boldsymbol{B}) &=& \\lambda \\boldsymbol{A} + \\lambda \\boldsymbol{B} \\\\\n",
    "(\\lambda + \\mu) \\boldsymbol{A} &=& \\lambda \\boldsymbol{A} + \\mu \\boldsymbol{A} \\\\\n",
    "\\lambda (\\mu \\boldsymbol{A}) &=& (\\lambda \\mu) \\boldsymbol{A}\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    ">The __first result__ states that you will get the same answer if you add two matrices together, and then multiply them by a scalar, as if you individually multiplied each matrix by the scalar and then added them together.\n",
    "\n",
    ">The __second result__ states that if you add two scalars together and then multiply the result by a matrix it gives the same answer as if you individually multiplied the matrix separately by each scalar and added the result.\n",
    "\n",
    ">The __third result__ states that the order of scalar multiplication does not matter. If you multiply the matrix by one scalar, and then multiply the result by another scalar it gives the same answer as if you first multiplied both scalars together and then by the matrix.\n",
    "\n",
    "All of these results follow from the simple rules of scalar multiplication and addition.\n",
    "\n",
    "#### <a href=\"https://en.wikipedia.org/wiki/Hadamard_product_(matrices)\" target=\"_top\">Hadamard Product</a>\n",
    "\n",
    "It is possible to define element-wise multiplication of matrices, which differs from the definition of matrix-matrix multiplication above. The __Hadamard product__ of two matrices $\\boldsymbol{A}=[a_{ij}]_{m \\times n}$ and $\\boldsymbol{B}=[b_{ij}]_{m \\times n}$ is denoted by $\\boldsymbol{A} \\odot \\boldsymbol{B}$ and calculated by the following expression:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\boldsymbol{A} \\odot \\boldsymbol{B} = [a_{ij} b_{ij}]_{m \\times n}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "That is, the elements of the new matrix are simply given as the scalar multiples of each of the elements from the other matrices. \n",
    "\n",
    ">Note that since scalar multiplication is __commutative__ so is the Hadamard Product, unlike normal matrix-matrix multiplication.\n",
    "\n",
    "When might it be necessary to use the Hadamard product? In fact such a process has wide applications, including correcting codes in satellite transmissions and cryptography, signal processing as well as lossy compression algorithms for images in JPEG format.\n",
    "\n",
    "#### [Dot Product](https://en.wikipedia.org/wiki/Dot_product)\n",
    "\n",
    "An important special case of matrix-matrix multiplication occurs between two vectors and is known as the __dot product__. It has a deep relationship with geometry and is useful in all areas of the physical and computational sciences.\n",
    "\n",
    "We have to be extremely careful here in regards to notation. I am being particularly precise about this definition, which may be unusual to some of you who have it seen it written before. In particular the dot product is really only defined as a true matrix-matrix multiplication, where one of the vectors is a row \"matrix\" and the other a column \"matrix\". However, you will often see a slight \"notational abuse\" where it is defined for any two vectors (row or column).\n",
    "\n",
    "The usual notation for a dot product between two -dimensional vectors $\\boldsymbol{x}, \\boldsymbol{y} \\in \\mathbb{R}^n$ is $\\boldsymbol{x} \\cdot \\boldsymbol{y}$, which is where the name of the operation comes from. However in more advanced textbooks (particularly the popular graduate level statistics, machine learning and deep learning texts) you will see it written as $\\boldsymbol{x}^T \\boldsymbol{y}$, where $T$ represents the transpose of $\\boldsymbol{x}$.\n",
    "\n",
    "This is because $\\boldsymbol{x}$ and $\\boldsymbol{y}$ are usually considered to both be column vectors. A matrix-matrix multiplication between two column vectors is not defined. Hence, one of the vectors needs to be transposed into a row vector such that the matrix-matrix multiplication is properly defined. Hence you will see the $\\boldsymbol{x}^T \\boldsymbol{y}$ notation frequently in more advanced textbooks and papers. Now we can proceed with the definition!\n",
    "\n",
    "Given two column vectors $\\boldsymbol{x}, \\boldsymbol{y} \\in \\mathbb{R}^n$\n",
    " it is possible to define the __dot product__ (or __scalar product__) between them by taking the transpose of one to form a product that is defined within the rules of matrix-matrix multiplication. Such a product produces a scalar value and is commutative:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\boldsymbol{x}^T \\boldsymbol{y} = \\sum^n_{i=1} x_i y_i = \\boldsymbol{y}^T \\boldsymbol{x}\n",
    "\\end{equation}\n",
    "$$\n",
    " \n",
    "The dot product has an important geometric meaning. It is the product of the [Euclidean lengths](https://en.wikipedia.org/wiki/Euclidean_distance) of the two vectors and the cosine of the angle between them. In subsequent articles the concept of __norms__ will be introduced, at which point this definition will be formalised.\n",
    "\n",
    "Another way to think about a dot product is that if we take the dot product of a vector with itself it is the square of the length of the vector:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\boldsymbol{x}^T \\boldsymbol{x} = \\sum^n_{i=1} x_i x_i = \\sum^n_{i=1} x_i^2\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    " \n",
    " \n",
    "Hence to find the (Euclidean) length of a vector you can simply take the square root of the dot product of the vector, $\\sqrt{\\boldsymbol{x}^T \\boldsymbol{x}}$.\n",
    "\n",
    "#### Sources: \n",
    "* [Matrix Algebra - Linear Algebra for Deep Learning (Part 2)](https://www.quantstart.com/articles/matrix-algebra-linear-algebra-for-deep-learning-part-2/)\n",
    "* [Basic Linear Algebra for Deep Learning](https://towardsdatascience.com/linear-algebra-for-deep-learning-f21d7e7d7f23)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Inversion\n",
    "\n",
    "In this section we will deal with matrix inversion. The importance of matrix inversion as a fundamental technique lies in the fact that it can help us solve these simultaneous linear equations. It is an important method in statistics and machine learning. Accordingly, we will describe matrix inversion through the concept of solving simultaneous linear equations.\n",
    "\n",
    "The need to solve __simultaneous linear equations__ arises in many fields of applied science. Within physics and engineering numerically simulating fluid flows on a computer requires the solution of simultaneous linear equations. In quantitative finance they are utilised to solve the [Black-Scholes PDE](https://www.quantstart.com/articles/Deriving-the-Black-Scholes-Equation/), which is necessary for certain types of options pricing.\n",
    "\n",
    "### [Simultaneous Linear Equations](https://en.wikipedia.org/wiki/Simultaneous_equations_model)\n",
    "\n",
    "At the begin we will look on a example of a small set of simultaneous linear equations:\n",
    "\n",
    "$$\\begin{eqnarray}\n",
    "x + 2y + 4z &=& 10 \\\\\n",
    "3x + y - 5z &=& -8 \\\\\n",
    "4x - 3y + 7z &=& 4\n",
    "\\end{eqnarray}\n",
    "$$\n",
    " \n",
    "The usual goal is to find the values of $x$, $y$ and $z$ that satisfy these equations. In the above case with only three equations some relatively simple algebraic manipulation will provide the answer.\n",
    "\n",
    "Once the number of equations grows significantly larger however it can becomes notationally unwieldy to write the equations in this manner.\n",
    "\n",
    "An alternative is to write such systems compactly using matrix notation. By setting $\\boldsymbol{x} = [x, y, z]^T$, $\\boldsymbol{b} = [10, -8, 4]^T$ and the matrix of coefficients $\\boldsymbol{A}$ by the following:\n",
    "\n",
    "$$ \\boldsymbol{A} = \\left[ \\begin{array}{ccc}\n",
    "1 & 2 & 4 \\\\\n",
    "3 & 1 & -5 \\\\\n",
    "4 & -3 & 7\n",
    "\\end{array} \\right] \n",
    "$$\n",
    " \n",
    "It is possible to write the above system as:\n",
    "\n",
    "$$\n",
    " \\left[ \\begin{array}{ccc}\n",
    "1 & 2 & 4 \\\\\n",
    "3 & 1 & -5 \\\\\n",
    "4 & -3 & 7\n",
    "\\end{array} \\right]\n",
    "\\left[ \\begin{array}{c}\n",
    "x \\\\\n",
    "y \\\\\n",
    "z\n",
    "\\end{array} \\right] =\n",
    "\\left[ \\begin{array}{c}\n",
    "10 \\\\\n",
    "-8 \\\\\n",
    "4\n",
    "\\end{array} \\right]\n",
    "$$\n",
    " \n",
    "Or, more compactly:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{A} \\boldsymbol{x} = \\boldsymbol{b}\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    " \n",
    "Note the bold typeface emphasising that the terms in the equation are vector and matrix quantities rather than scalar quantities.\n",
    "\n",
    "If this were an algebraic equation of the form $A x = b$, where $A, x, b \\in \\mathbb{R}$, that is, the terms are all scalar quantities, with $A \\neq 0$ then this could be solved for  simply by dividing through by $A$:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "x = \\frac{b}{A} = \\frac{1}{A} b = A^{-1} b\n",
    "\\end{equation}\n",
    "$$ \n",
    " \n",
    "In the final equality of this line it is shown that $x = A^{-1} b$. One might now ask whether it is possible to solve the matrix equation $\\boldsymbol{A} \\boldsymbol{x} = \\boldsymbol{b}$ via some form of analagous expression $\\boldsymbol{x} = \\boldsymbol{A}^{-1} \\boldsymbol{b}$?\n",
    "\n",
    "Since the operation of dividing through by a matrix is undefined the answer lies in defining the matrix $\\boldsymbol{A}^{-1}$, which is known as the __matrix inverse__ of $\\boldsymbol{A}$. In order to do so however it is necessary to introduce some other mathematical tools.\n",
    "\n",
    "### [Kronecker Delta and Indentity Matrices](https://de.wikipedia.org/wiki/Kronecker-Delta)\n",
    "\n",
    "The __Kronecker delta__ is a mathematical function of two variables $i,j$ with the following values:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\delta_{ij} =\n",
    "    \\begin{cases}\n",
    "            1, &         \\text{if } i=j,\\\\\n",
    "            0, &         \\text{if } i\\neq j.\n",
    "    \\end{cases}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    " \n",
    "Essentially, the Kronecker delta equals one when $i$ and $j$ are equal and zero otherwise.\n",
    "\n",
    "This function can be utilised to define a new square matrix where each element in the matrix at position $i,j$ is equal to the Kronecker delta of that value.\n",
    "\n",
    "Mathematically this is written compactly as $\\boldsymbol{I}_n=[\\delta_{ij}]_{n \\times n}$. The matrix itself will have the following form:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\boldsymbol{I}_n=\\begin{bmatrix}\n",
    "  \\kern4pt 1 & 0 & 0 & \\dots & 0 \\kern4pt \\\\\n",
    "  \\kern4pt 0 & 1 & 0 & \\dots & 0 \\kern4pt \\\\\n",
    "  \\kern4pt 0 & 0 & 1 & \\dots & 0 \\kern4pt \\\\\n",
    "  \\kern4pt \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\kern4pt \\\\\n",
    "  \\kern4pt 0 & 0 & 0 & \\dots & 1 \\kern4pt \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "$$\n",
    " \n",
    "In particular all elements of the matrix are zero except those on the __diagonal of the matrix__, which are __equal to one__. This is because the diagonal elements represent the case where $i=j$ and the Kronecker delta equals one at these values.\n",
    "\n",
    "This type of matrix is known as the __identity matrix__ with dimensionality $n$. The identity matrix possesses the unique property that when it left- or right-multiplies any square matrix $\\boldsymbol{A} \\in \\mathbb{R}^{n \\times n}$ it leaves $\\boldsymbol{A}$ unchanged:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\boldsymbol{I}_n \\boldsymbol{A} = \\boldsymbol{A} \\boldsymbol{I}_n = \\boldsymbol{A}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "This now allows us to define the __matrix inverse__ $\\boldsymbol{A}^{-1}$. The matrix inverse is precisely the matrix that when left- or right-multiplied to $\\boldsymbol{A}$ produces the identity matrix:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\boldsymbol{A}^{-1} \\boldsymbol{A} = \\boldsymbol{I}_n = \\boldsymbol{A} \\boldsymbol{A}^{-1}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "In order to gain some intuition as to why this is so consider the following familiar rules of multiplication for an equivalent scalar algebraic equation:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\frac{a}{a} = \\frac{1}{a} a = a^{-1} a = 1 = a a^{-1} = a \\frac{1}{a} = \\frac{a}{a}\n",
    "\\end{equation}\n",
    "$$\n",
    " \n",
    "In particular $a^{-1} a = 1 = a a^{-1}$. The value $1$ can be interpreted as a $1 \\times 1$ matrix with a single value.\n",
    "\n",
    "### Solving the Matrix Equation\n",
    "\n",
    "With these definitions it is now possible to solve the original equation $\\boldsymbol{A} \\boldsymbol{x} = \\boldsymbol{b}$:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{A} \\boldsymbol{x} &=& \\boldsymbol{b} \\\\\n",
    "\\boldsymbol{A}^{-1} \\boldsymbol{A} \\boldsymbol{x} &=& \\boldsymbol{A}^{-1} \\boldsymbol{b} \\\\\n",
    "\\boldsymbol{I}_n \\boldsymbol{x} &=& \\boldsymbol{A}^{-1} \\boldsymbol{b} \\\\\n",
    "\\boldsymbol{x} &=& \\boldsymbol{A}^{-1} \\boldsymbol{b}\\label{eqn-matrix-inverse}\n",
    "\\end{eqnarray}\n",
    "$$\n",
    " \n",
    "Firstly the equation is left-multiplied by the inverse matrix $\\boldsymbol{A}^{-1}$. The term $\\boldsymbol{A}^{-1} \\boldsymbol{A}$ can then be set to the identify matrix $\\boldsymbol{I}_n$.\n",
    "\n",
    "However we noted that any vector or matrix multiplied by the identify matrix is left unchanged. Hence we can remove the reference to $\\boldsymbol{I}_n$ and leave an expression for $\\boldsymbol{x}$ in terms of the matrix inverse $\\boldsymbol{A}^{-1}$ and the vector $\\boldsymbol{b}$.\n",
    "\n",
    "It is now possible to solve the equation $\\boldsymbol{x} = \\boldsymbol{A}^{-1} \\boldsymbol{b}$ for the unknown vector $\\boldsymbol{x}$ assuming that an appropriate $\\boldsymbol{A}^{-1}$ exists.\n",
    "\n",
    "However we have yet to describe how to go about actually calculating $\\boldsymbol{A}^{-1}$ or whether it is even possible to do so.\n",
    "\n",
    "### Solving the Matrix Equation\n",
    "\n",
    "A common method to find the inverse of a matrix (if it exists) is to use a technique known as __[Gauss-Jordan elimination](https://en.wikipedia.org/wiki/Gaussian_elimination)__ (or Gaussian Elimination). However for a large $n \\times n$-dimensional matrix this is an expensive and inefficient mechanism for finding an inverse.\n",
    "\n",
    "The cost of the operation scales cubically with the size of the matrix. That is it has an arithmetic complexity of $\\mathcal{O}(n^3)$. This can be a significant problem if many such inverse operations need to be carried out.\n",
    "\n",
    "For most applications in applied science, quantitative finance and deep learning it is sufficient to __approximate__ the solution $\\boldsymbol{x}$ to within a certain tolerance rather than directly seeking the exact value.\n",
    "\n",
    "There are many less computationally intensive algorithms that are effective enough to provide the necessary accuracy.\n",
    "\n",
    "These algorithms consist either of a well-optimised version of Gauss-Jordan elimination for [particular structured matrices](https://en.wikipedia.org/wiki/Partial_differential_equation) or make use of an [iterative approach](https://en.wikipedia.org/wiki/Iterative_method) that approximates  to a certain precision in a finite number of iterations.\n",
    "\n",
    "The need to effectively approximate $\\boldsymbol{x}$ in the above matrix equation has lead to the development of an entire subfield of mathematics known as __Numerical Linear Algebra__.\n",
    "\n",
    "Certain __matrix factorisations__ can be extremely useful in more efficiently solving this equation. These factorisations will be discussed at length in subsequent articles.\n",
    "\n",
    "\n",
    "#### Sources: \n",
    "* [Matrix Algebra - Linear Algebra for Deep Learning (Part 3)](https://www.quantstart.com/articles/matrix-inversion-linear-algebra-for-deep-learning-part-3/)\n",
    "* [Data Science | Solving Linear Equations](https://www.geeksforgeeks.org/data-science-solving-linear-equations/)\n",
    "* [A comprehensive beginners guide to Linear Algebra for Data Scientists](https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
